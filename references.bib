@misc{CopyrightAct,
	title = {Intellectual {Property} {Act}, {No.36} of 2003},
	author = {{Parliament of the democratic socialist republic of Sri Lanka}}
}

@article{Nishan,
	title = {Radio {Broadcast} {Monitoring} to {Ensure} {Copyright} {Ownership}},
	volume = {11},
	issn = {2550-2794, 1800-4156},
	url = {https://icter.sljol.info/article/10.4038/icter.v11i1.7193/},
	doi = {10.4038/icter.v11i1.7193},
	abstract = {In this research, we provide a way to protect copyright ownership of multimedia objects like songs. Protecting ownership is very important when we are working in a corporative environment. If someone tries to misuse others’ property it is an illegal action. But capturing theses kind of actions especially on electronic objects are very difficult. There are several ways to share copyright ownership with others. We can purchase others’ property and use them under conditions provided by the owner.},
	language = {en},
	number = {1},
	urldate = {2019-03-24},
	journal = {International Journal on Advances in ICT for Emerging Regions (ICTer)},
	author = {Senevirathna, E. D. N. W. and Jayaratne, K. L.},
	month = aug,
	year = {2018},
	pages = {1}
}

@incollection{SerraBook,
address = {Berlin, Heidelberg},
title = {Audio {Cover} {Song} {Identification} and {Similarity}: {Background}, {Approaches}, {Evaluation}, and {Beyond}},
volume = {274},
isbn = {978-3-642-11673-5 978-3-642-11674-2},
shorttitle = {Audio {Cover} {Song} {Identification} and {Similarity}},
url = {http://link.springer.com/10.1007/978-3-642-11674-2_14},
urldate = {2019-02-27},
booktitle = {Advances in {Music} {Information} {Retrieval}},
publisher = {Springer Berlin Heidelberg},
author = {Serrà, Joan and Gómez, Emilia and Herrera, Perfecto},
editor = {Kacprzyk, Janusz and Raś, Zbigniew W. and Wieczorkowska, Alicja A.},
year = {2010},
doi = {10.1007/978-3-642-11674-2_14},
pages = {307--332}
}

@article{Kehtarnavaz2008,
	author = {Kehtarnavaz, Nasser},
	doi = {10.1016/b978-0-12-374490-6.00007-6},
	journal = {Digital Signal Processing System Design},
	pages = {175--196},
	title = {{Frequency Domain Processing}},
	volume = {1},
	year = {2008}
}

@article{Ke2005,
	author = {Ke, Yan and Hoiem, Derek and Sukthankar, Rahul},
	doi = {10.1109/CVPR.2005.105},
	isbn = {0769523722},
	journal = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
	pages = {597--604},
	title = {{Computer vision for music identification}},
	volume = {I},
	year = {2005}
}

@techreport{Sears2006,
	abstract = {Application designers must decide whether to store large objects (BLOBs) in a filesystem or in a database. Generally, this decision is based on factors such as application simplicity or manageability. Often, system performance affects these factors. Folklore tells us that databases efficiently handle large numbers of small objects, while filesystems are more efficient for large objects. Where is the break-even point? When is accessing a BLOB stored as a file cheaper than accessing a BLOB stored as a database record? The simple answer is: BLOBs smaller than 256KB are more efficiently handled by a database, while a filesystem is more efficient for those greater than 1MB. Of course, this will vary between different databases and filesystems. By measuring the performance of a storage server that mimics common workloads we found that the break-even point depends on many factors. However, our experiments suggest that storage age, the ratio of bytes in deleted objects to bytes in live objects, is dominant. As storage age increases, fragmentation tends to increase. The filesystem we study has better fragmentation control than the database we used, suggesting the database system would benefit from incorporating ideas from filesystem design. Conversely, filesystem performance may be improved by using database techniques to handle many small files. Surprisingly, for these studies, when average object size is held constant, the distribution of object sizes did not significantly affect performance. We also found that, in addition to low percentage free space, a low ratio of free space to average object size leads to fragmentation and performance degradation.},
	author = {Sears, Russell and {Van Ingen}, Catharine and Gray, Jim},
	title = {{To BLOB or Not To BLOB: Large Object Storage in a Database or a Filesystem?}},
	year = {2006}
}

@techreport{Muja,
	abstract = {For many computer vision problems, the most time consuming component consists of nearest neighbor matching in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question, "What is the fastest approximate nearest-neighbor algorithm for my data?" Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized k-d trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection.},
	author = {Muja, Marius and Lowe, David G},
	title = {{Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration}}
}

@article{Lowe2004,
	author = {Lowe, David G.},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	issn = {09205691},
	journal = {International Journal of Computer Vision},
	keywords = {Image matching,Invariant features,Object recognition,Scale invariance},
	number = {2},
	pages = {91--110},
	title = {{Distinctive image features from scale-invariant keypoints}},
	volume = {60},
	year = {2004}
}

@inproceedings{query_by_humming,
address = {San Francisco, California, United States},
title = {Query by humming: musical information retrieval in an audio database},
isbn = {978-0-89791-751-3},
shorttitle = {Query by humming},
url = {http://portal.acm.org/citation.cfm?doid=217279.215273},
doi = {10.1145/217279.215273},
abstract = {The emergence of audio and video data types in databases will require new information retrieval methods adapted to the speciﬁc characteristics and needs of these data types. An effective and natural way of querying a musical audio database is by humming the tune of a song. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Relevant difﬁculties involved with tracking pitch are enumerated, along with the approach we followed, and the performance results of system indicating its effectiveness are presented.},
language = {en},
urldate = {2019-08-02},
booktitle = {Proceedings of the third {ACM} international conference on {Multimedia}  - {MULTIMEDIA} '95},
publisher = {ACM Press},
author = {Ghias, Asif and Logan, Jonathan and Chamberlin, David and Smith, Brian C.},
year = {1995},
pages = {231--236},
file = {Ghias et al. - 1995 - Query by humming musical information retrieval in.pdf:C\:\\Users\\pasin\\Zotero\\storage\\URGT34HU\\Ghias et al. - 1995 - Query by humming musical information retrieval in.pdf:application/pdf}
}

@article{comparative_query_by_humming,
	title = {A comparative evaluation of search techniques for query-by-humming using the {MUSART} testbed},
	volume = {58},
	issn = {15322882, 15322890},
	url = {http://doi.wiley.com/10.1002/asi.20532},
	doi = {10.1002/asi.20532},
	abstract = {Query-by-Humming systems offer content-based searching for melodies and require no special musical training or knowledge. Many such systems have been built, but there has not been much useful evaluation and comparison in the literature due to the lack of shared databases and queries. The MUSART project testbed allows various search algorithms to be compared using a shared framework that automatically runs experiments and summarizes results. Using this testbed, we compared algorithms based on string alignment, melodic contour matching, a hidden Markov model, n-grams, and CubyHum. Retrieval performance is very sensitive to distance functions and the representation of pitch and rhythm, which raises questions about some previously published conclusions. Some algorithms are particularly sensitive to the quality of queries. Our queries, which are taken from human subjects in a fairly realistic setting, are quite difficult, especially for n-gram models. Finally, simulations on query-byhumming performance as a function of database size indicate that retrieval performance falls only slowly as the database size increases.},
	language = {en},
	number = {5},
	urldate = {2019-08-02},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Dannenberg, Roger B. and Birmingham, William P. and Pardo, Bryan and Hu, Ning and Meek, Colin and Tzanetakis, George},
	month = mar,
	year = {2007},
	pages = {687--701},
}

@inproceedings{STFT_and_SIFT,
address = {Firenze, Italy},
title = {A novel audio fingerprinting method robust to time scale modification and pitch shifting},
isbn = {978-1-60558-933-6},
url = {http://dl.acm.org/citation.cfm?doid=1873951.1874130},
doi = {10.1145/1873951.1874130},
abstract = {A novel audio ﬁngerprinting method that is highly robust to Time Scale Modiﬁcation (TSM) and pitch shifting is proposed. Instead of simply employing spectral or tempo-related features, our system is based on computer-vision techniques. We transform each 1-D audio signal into a 2-D image and treat TSM and pitch shifting of the audio signal as stretch and translation of the corresponding image. Robust local descriptors are extracted from the image and matched against those of the reference audio signals. Experimental results show that our system is highly robust to various audio distortions, including the challenging TSM and pitch shifting.},
language = {en},
urldate = {2019-06-28},
booktitle = {Proceedings of the international conference on {Multimedia} - {MM} '10},
publisher = {ACM Press},
author = {Zhu, Bilei and Li, Wei and Wang, Zhurong and Xue, Xiangyang},
year = {2010},
pages = {987}
}

@article{fusion_similarity,
	title = {Fusing similarity functions for cover song identification},
	volume = {77},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-017-4456-9},
	doi = {10.1007/s11042-017-4456-9},
	abstract = {Cover Song Identification (CSI) technique, refers to the process of identifying an alternative version, performance, rendition, or recording of a previously recorded musical composition by measuring and modeling the musical similarity between them quantitatively and objectively. However, it is not possible to describe the similarity between tracks comprehensively and reliably with only one similarity function. In this paper, the Similarity Network Fusion (SNF) technique, which was originally proposed for combining different kernels for predicting drug-target interactions, is adopted to fuse different similarities based on the same descriptor and different similarity functions. First, the Harmonic Pitch Class Profile (HPCP) is extracted from each track. Next, the similarities, in terms of Qmax and Dmax measures, between the HPCP descriptors of any two tracks are calculated, respectively. Then, the track-by-track similarity networks based on Qmax and on Dmax similarity are constructed separately and then fused into one network by SNF. Finally, the fused similarities obtained from the fused similarity network are adopted to train a classifier, which can then be used to identify whether the input two tracks belong to reference/cover or reference/non-cover pair. Experimental results on Covers80 (http://labrosa.ee. columbia.edu/projects/coversongs/covers80/), subset of SecondHandSongs (SHS) (http:// labrosa.ee.columbia.edu/millionsong/secondhand), and the Mixed Collection and Mazurka Cover Collection provided by MIREX (http://www.music-ir.org/mirex/wiki/2016:Audio Cover Song Identification) demonstrate that the proposed scheme performs comparably with or even better than state-of-the-art CSI schemes.},
	language = {en},
	number = {2},
	urldate = {2019-07-08},
	journal = {Multimedia Tools and Applications},
	author = {Chen, Ning and Li, Wei and Xiao, Haidong},
	month = jan,
	year = {2018},
	pages = {2629--2652},
	file = {Chen et al. - 2018 - Fusing similarity functions for cover song identif.pdf:C\:\\Users\\pasin\\Zotero\\storage\\YL4ESW2D\\Chen et al. - 2018 - Fusing similarity functions for cover song identif.pdf:application/pdf}
}

@article{combined_classifiers,
	title = {Combining {Features} for {Cover} {Song} {Identification}},
	abstract = {In this paper, we evaluate a set of methods for combining features for cover song identiﬁcation. We ﬁrst create multiple classiﬁers based on global tempo, duration, loudness, beats and chroma average features, training a random forest for each feature. Subsequently, we evaluate standard combination rules for merging these single classiﬁers into a composite classiﬁer based on global features. We further obtain two higher level classiﬁers based on chroma features: one based on comparing histograms of quantized chroma features, and a second one based on computing cross-correlations between sequences of chroma features, to account for temporal information. For combining the latter chroma-based classiﬁers with the composite classiﬁer based on global features, we use standard rank aggregation methods adapted from the information retrieval literature. We evaluate performance with the Second Hand Song dataset, where we quantify performance using multiple statistics. We observe that each combination rule outperforms single methods in terms of the total number of identiﬁed queries. Experiments with rank aggregation methods show an increase of up to 23.5 \% of the number of identiﬁed queries, compared to single classiﬁers.},
	language = {en},
	author = {Osmalsky, J and Embrechts, J -J and Foster, P and Dixon, S},
	pages = {7},
	file = {Osmalsky et al. - Combining Features for Cover Song Identification.pdf:C\:\\Users\\pasin\\Zotero\\storage\\8TM3DZQ7\\Osmalsky et al. - Combining Features for Cover Song Identification.pdf:application/pdf}
}

@incollection{cross-similarity_measure,
address = {Cham},
title = {Cross-{Similarity} {Measurement} of {Music} {Sections}: {A} {Framework} for {Large}-scale {Cover} {Song} {Identification}},
volume = {63},
isbn = {978-3-319-50208-3 978-3-319-50209-0},
shorttitle = {Cross-{Similarity} {Measurement} of {Music} {Sections}},
url = {http://link.springer.com/10.1007/978-3-319-50209-0_19},
abstract = {For large-scale cover song identiﬁcation, most previous works take a single feature vector as the representation of a song. Although this approach ensures structure invariance, it may cause overcorrection since it totally neglects the structure feature of the song. To address this problem, we put forward a novel framework for large-scale cover song identiﬁcation based on music structure segmentation, aiming at matching the irrelevant sections and ignoring the irrelevant ones. In our implementation, we apply the average and weighted average methods to integrating similarities of section pairs. We evaluate the proposed framework based on three representative previous methods, including 2D Fourier magnitude coeﬃcients, chord proﬁles, and cognition-inspired descriptors. The experimental results show that the all the three methods in our framework signiﬁcantly outperform those in their original works.},
language = {en},
urldate = {2019-07-08},
booktitle = {Advances in {Intelligent} {Information} {Hiding} and {Multimedia} {Signal} {Processing}},
publisher = {Springer International Publishing},
author = {Cai, Kang and Yang, Deshun and Chen, Xiaoou},
editor = {Pan, Jeng-Shyang and Tsai, Pei-Wei and Huang, Hsiang-Cheh},
year = {2017},
doi = {10.1007/978-3-319-50209-0_19},
pages = {151--158}
}

@incollection{two_layer_model,
address = {Cham},
title = {Efficient {Two}-{Layer} {Model} {Towards} {Cover} {Song} {Identification}},
volume = {10705},
isbn = {978-3-319-73599-3 978-3-319-73600-6},
url = {http://link.springer.com/10.1007/978-3-319-73600-6_11},
language = {en},
urldate = {2019-07-08},
booktitle = {{MultiMedia} {Modeling}},
publisher = {Springer International Publishing},
author = {Xu, Xiaoshuo and Cheng, Yao and Chen, Xiaoou and Yang, Deshun},
editor = {Schoeffmann, Klaus and Chalidabhongse, Thanarat H. and Ngo, Chong Wah and Aramvith, Supavadee and O’Connor, Noel E. and Ho, Yo-Sung and Gabbouj, Moncef and Elgammal, Ahmed},
year = {2018},
doi = {10.1007/978-3-319-73600-6_11},
pages = {118--128}
}

@article{large-scale,
title = {Large-{Scale} {Cover} {Song} {Detection} in {Digital} {Music} {Libraries} {Using} {Metadata}, {Lyrics} and {Audio} {Features}},
url = {http://arxiv.org/abs/1808.10351},
language = {en},
urldate = {2019-07-08},
journal = {arXiv:1808.10351 [cs]},
author = {Correya, Albin Andrew and Hennequin, Romain and Arcos, Mickaël},
month = aug,
year = {2018},
note = {arXiv: 1808.10351},
keywords = {Computer Science - Information Retrieval, Computer Science - Multimedia}
}

@inproceedings{deep_hashing,
	address = {San Jose, CA, USA},
	title = {Supervised {Deep} {Hashing} for {Highly} {Efficient} {Cover} {Song} {Detection}},
	isbn = {978-1-72811-198-8},
	url = {https://ieeexplore.ieee.org/document/8695391/},
	doi = {10.1109/MIPR.2019.00049},
	language = {en},
	urldate = {2019-07-08},
	booktitle = {2019 {IEEE} {Conference} on {Multimedia} {Information} {Processing} and {Retrieval} ({MIPR})},
	publisher = {IEEE},
	author = {Ye, Zhaoqin and Choi, Jaeyoung and Friedland, Gerald},
	month = mar,
	year = {2019},
	pages = {234--239}
	}

@article{ensemble_based_detection,
title = {Ensemble-based cover song detection},
url = {http://arxiv.org/abs/1905.11700},
language = {en},
urldate = {2019-07-08},
journal = {arXiv:1905.11700 [cs, eess]},
author = {Sarfati, Marc and Hu, Anthony and Donier, Jonathan},
month = may,
year = {2019},
note = {arXiv: 1905.11700}
}

@article{timbral_shape_sequences,
title = {Cover {Song} {Identification} with {Timbral} {Shape} {Sequences}},
url = {http://arxiv.org/abs/1507.05143},
abstract = {We introduce a novel low level feature for identifying cover songs which quantiﬁes the relative changes in the smoothed frequency spectrum of a song. Our key insight is that a sliding window representation of a chunk of audio can be viewed as a time-ordered point cloud in high dimensions. For corresponding chunks of audio between diﬀerent versions of the same song, these point clouds are approximately rotated, translated, and scaled copies of each other. If we treat MFCC embeddings as point clouds and cast the problem as a relative shape sequence, we are able to correctly identify 42/80 cover songs in the “Covers 80” dataset. By contrast, all other work to date on cover songs exclusively relies on matching note sequences from Chroma derived features.},
language = {en},
urldate = {2019-07-08},
journal = {arXiv:1507.05143 [cs]},
author = {Tralie, Christopher J. and Bendich, Paul},
month = jul,
year = {2015},
note = {arXiv: 1507.05143},
keywords = {Computer Science - Sound}
}

@inproceedings{Chord,
title = {Audio-{Based} {Cover} {Song} {Retrieval} {Using} {Approximate} {Chord} {Sequences}: {Testing} {Shifts}, {Gaps}, {Swaps} and {Beats}.},
volume = {7},
shorttitle = {Audio-{Based} {Cover} {Song} {Retrieval} {Using} {Approximate} {Chord} {Sequences}},
booktitle = {{ISMIR}},
publisher = {Citeseer},
author = {Bello, Juan Pablo},
year = {2007},
pages = {239--244}
}

@article{PCP,
title = {Cover song identification with {IF}-{F}0 pitch class profiles},
journal = {MIREX extended abstract},
author = {Egorov, Alexey and Linetsky, Gene},
year = {2008}
}

@inproceedings{Energy,
title = {{ARTHUR}: {Retrieving} {Orchestral} {Music} by {Long}-{Term} {Structure}.},
shorttitle = {{ARTHUR}},
booktitle = {{ISMIR}},
author = {Foote, Jonathan},
year = {2000},
file = {Full Text:C\:\\Users\\pasin\\Zotero\\storage\\ITW74QZM\\Foote - 2000 - ARTHUR Retrieving Orchestral Music by Long-Term S.pdf:application/pdf}
}

@inproceedings{KeyTemplates,
	title = {Tonal {Similarity} from {Audio} {Using} a {Template} {Based} {Attractor} {Model}.},
	booktitle = {{ISMIR}},
	publisher = {Citeseer},
	author = {Özgür İzmirli},
	year = {2005},
	pages = {540--545}
}

@inproceedings{Melody,
title = {A {Mid}-level {Melody}-based {Representation} for {Calculating} {Audio} {Similarity}.},
booktitle = {{ISMIR}},
author = {Marolt, Matija},
year = {2006},
pages = {280--285}
}

@inproceedings{PBFV,
	title = {Fast music retrieval using polyphonic binary feature vectors},
	volume = {1},
	booktitle = {Proceedings. {IEEE} {International} {Conference} on {Multimedia} and {Expo}},
	publisher = {IEEE},
	author = {Nagano, Hidehisa and Kashino, Kunio and Murase, Hiroshi},
	year = {2002},
	pages = {101--104}
}

@article{Zhang2011,
author = {Zhang, Xiu and Zhu, Bilei and Li, Linwei and Li, Wei and Li, Xiaoqiang and Wang, Wei and Lu, Peizhong and Zhang, Wenqiang},
doi = {10.1186/s13636-015-0050-0},
journal = {EURASIP Journal on Audio, Speech, and Music Processing},
mendeley-groups = {Undergraduate Research,Undergraduate Research/Rearch Paper References},
pages = {6},
title = {{SIFT-based local spectrogram image descriptor: a novel feature for robust music identification}},
volume = {2015},
year = {2011}
}
